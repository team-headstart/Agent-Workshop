{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![Img](https://app.theheadstarter.com/static/hs-logo-opengraph.png)\n",
        "\n",
        "# Headstarter AI Agent Workshop\n",
        "# Please go to the end of the document to the cell where all the code is included and run it all at once.\n",
        "\n",
        "#### **Skills: OpenAI, Groq, Llama, OpenRouter**\n",
        "\n",
        "## **To Get Started:**\n",
        "1. [Get your Groq API Key](https://console.groq.com/keys)\n",
        "2. [Get your OpenRouter API Key](https://openrouter.ai/settings/keys)\n",
        "3. [Get your OpenAI API Key](https://platform.openai.com/api-keys)\n",
        "\n",
        "\n",
        "### **Interesting Reads**\n",
        "- [Sam Altman's Blog Post: The Intelligence Age](https://ia.samaltman.com/)\n",
        "- [What LLMs cannot do](https://ehudreiter.com/2023/12/11/what-llms-cannot-do/)\n",
        "- [Chain of Thought Prompting](https://www.promptingguide.ai/techniques/cot)\n",
        "- [Why ChatGPT can't count the number of r's in the word strawberry](https://prompt.16x.engineer/blog/why-chatgpt-cant-count-rs-in-strawberry)\n",
        "\n",
        "\n",
        "## During the Workshop\n",
        "- [Any code shared during the workshop will be posted here](https://docs.google.com/document/d/1hPBJt_4Ihkj6v667fWxVjzwCMS4uBPdYlBLd2IqkxJ0/edit?usp=sharing)"
      ],
      "metadata": {
        "id": "nO8gHDbSAa4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install necessary libraries"
      ],
      "metadata": {
        "id": "Pt_SMGlvocqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install openai groq"
      ],
      "metadata": {
        "id": "eKvKwO8XAnPt",
        "collapsed": true
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "17uHqkljVxma"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up Groq, OpenRouter, & OpenAI clients"
      ],
      "metadata": {
        "id": "GjcgEeFaof1i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "pHbjDU_L__Vd"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import json\n",
        "from groq import Groq\n",
        "import json\n",
        "from typing import List, Dict, Any, Callable\n",
        "import ast\n",
        "import io\n",
        "import sys\n",
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "os.environ['GROQ_API_KEY'] = groq_api_key\n",
        "\n",
        "openrouter_api_key = userdata.get(\"OPENROUTER_API_KEY\")\n",
        "os.environ['OPENROUTER_API_KEY'] = openrouter_api_key\n",
        "\n",
        "openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "groq_client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
        "\n",
        "openrouter_client = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=os.getenv(\"OPENROUTER_API_KEY\")\n",
        ")\n",
        "\n",
        "openai_client = OpenAI(\n",
        "    base_url=\"https://api.openai.com/v1\",\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define functions to easily query and compare responses from OpenAI, Groq, and OpenRouter"
      ],
      "metadata": {
        "id": "AZF6uLpooj-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_llm_response(client, prompt, openai_model=\"o1-preview\", json_mode=False):\n",
        "\n",
        "    if client == \"openai\":\n",
        "\n",
        "        kwargs = {\n",
        "            \"model\": openai_model,\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "        }\n",
        "\n",
        "        if json_mode:\n",
        "            kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
        "\n",
        "        response = openai_client.chat.completions.create(**kwargs)\n",
        "\n",
        "    elif client == \"groq\":\n",
        "\n",
        "        try:\n",
        "            models = [\"llama-3.1-8b-instant\", \"llama-3.1-70b-versatile\", \"llama3-70b-8192\", \"llama3-8b-8192\", \"gemma2-9b-it\"]\n",
        "\n",
        "            for model in models:\n",
        "\n",
        "                try:\n",
        "                    kwargs = {\n",
        "                        \"model\": model,\n",
        "                        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "                    }\n",
        "                    if json_mode:\n",
        "                        kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
        "\n",
        "                    response = groq_client.chat.completions.create(**kwargs)\n",
        "\n",
        "                    break\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error: {e}\")\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "            kwargs = {\n",
        "                \"model\": \"meta-llama/llama-3.1-8b-instruct:free\",\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "            }\n",
        "\n",
        "            if json_mode:\n",
        "                kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
        "\n",
        "            response = openrouter_client.chat.completions.create(**kwargs)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid client: {client}\")\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "def evaluate_responses(prompt, reasoning_prompt=False, openai_model=\"o1-preview\"):\n",
        "  # Here, I could append the response from the webscraper ##########################################################################\n",
        "\n",
        "    if reasoning_prompt:\n",
        "        prompt = f\"{prompt}\\n\\n{reasoning_prompt}.\"\n",
        "\n",
        "    openai_response = get_llm_response(\"openai\", prompt, openai_model)\n",
        "    groq_response = get_llm_response(\"groq\", prompt)\n",
        "\n",
        "    print(f\"OpenAI Response: {openai_response}\")\n",
        "    print(f\"\\n\\nGroq Response: {groq_response}\")"
      ],
      "metadata": {
        "id": "BNYyDCNuAhbj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Which is greater, 9.11 or 9.9?\"\n",
        "# Allows the model to think critically\n",
        "reasoning_prompt = \"Understand the problem and devise a plan to solve it. Then solve the problem step by step\"\n",
        "evaluate_responses(prompt, reasoning_prompt, openai_model=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "j9V03RXNBd6j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ddfad47-0b04-4401-de11-fce00cb19eac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI Response: To determine which is greater between 9.11 and 9.9, we need to compare the numbers digit by digit starting from the left.\n",
            "\n",
            "9.11\n",
            "9.90\n",
            "\n",
            "Since both numbers have the same digit at the first decimal place, we move on to the second decimal place.\n",
            "\n",
            "9.11\n",
            "9.90\n",
            "\n",
            "Comparing the numbers at the second decimal place, we see that 9.9 is greater than 9.11.\n",
            "\n",
            "Therefore, 9.9 is greater than 9.11.\n",
            "\n",
            "\n",
            "Groq Response: To determine which number is greater between 9.11 and 9.9, we can use a simple comparison. Here's the step-by-step plan:\n",
            "\n",
            "1. Compare the whole numbers (9 in both cases).\n",
            "2. Compare the decimal parts (0.11 and 0.9).\n",
            "\n",
            "**Step-by-Step Solution**\n",
            "\n",
            "1. Compare the whole numbers:\n",
            "Both numbers have the same whole number part, which is 9.\n",
            "2. Compare the decimal parts:\n",
            "- The decimal part of 9.11 is 0.11.\n",
            "- The decimal part of 9.9 is 0.9.\n",
            "Since 0.9 is greater than 0.11, \n",
            "Therefore, 9.9 is greater than 9.11.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent Architecture"
      ],
      "metadata": {
        "id": "0w8IlZq49PAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![](https://mermaid.ink/img/pako:eNqFUslugzAQ_ZWRz8kPILUVCZClVdQmqdTK5ODCFFDAjry0iSD_XmOTJodK5WS_Zd7M4JZkIkcSkEKyQwnbKOVgvzf6qlDCi0F52sF4fA-hJ0IaGi24aIRREBbItacn9LlmnKPced2kR7s1aiO5AmU-NFN71cG0fRLiALqUwhQlbAbi7F1TVyuia2RKXItFDo5pmOnqBo5dhgcDmKFlmEaY2oE6SOgvwHgO8REzM5B_2n1kxYsOZtSrxSUocfkzf5m5y5zGX6w27Cqau3IDOpTU8gR3kLBa2Y4W7QqP-jLywzDywtnesd_NLbISHSxpUnFWQ8jVt_0b8VFLll0Tl66TR-q3DLfaf3vaSoMukYxIg7JhVW4fQdvbUqJLbDAlgT3mTO5TkvKz1TG7ks2JZyTQ1j0i5pDb9UYVs2-nIcFnP-YFjfPKNjqA5x9CM8YW?type=png)](https://mermaid.live/edit#pako:eNqFUslugzAQ_ZWRz8kPILUVCZClVdQmqdTK5ODCFFDAjry0iSD_XmOTJodK5WS_Zd7M4JZkIkcSkEKyQwnbKOVgvzf6qlDCi0F52sF4fA-hJ0IaGi24aIRREBbItacn9LlmnKPced2kR7s1aiO5AmU-NFN71cG0fRLiALqUwhQlbAbi7F1TVyuia2RKXItFDo5pmOnqBo5dhgcDmKFlmEaY2oE6SOgvwHgO8REzM5B_2n1kxYsOZtSrxSUocfkzf5m5y5zGX6w27Cqau3IDOpTU8gR3kLBa2Y4W7QqP-jLywzDywtnesd_NLbISHSxpUnFWQ8jVt_0b8VFLll0Tl66TR-q3DLfaf3vaSoMukYxIg7JhVW4fQdvbUqJLbDAlgT3mTO5TkvKz1TG7ks2JZyTQ1j0i5pDb9UYVs2-nIcFnP-YFjfPKNjqA5x9CM8YW)\n",
        "\n",
        "\n",
        "![agent_architecture_v2](https://github.com/user-attachments/assets/a65b6db9-bef1-4579-aed3-01444ce40544)"
      ],
      "metadata": {
        "id": "RxpUp2KED9hh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To create our AI Agent, we will define the following functions:\n",
        "\n",
        "1. **Planner:** This function takes a user's query and breaks it down into smaller, manageable subtasks. It returns these subtasks as a list, where each one is either a reasoning task or a code generation task.\n",
        "\n",
        "2. **Reasoner:** This function provides reasoning on how to complete a specific subtask, considering both the overall query and the results of any previous subtasks. It returns a short explanation on how to proceed with the current subtask.\n",
        "\n",
        "3. **Actioner:** Based on the reasoning provided for a subtask, this function decides whether the next step requires generating code or more reasoning. It then returns the chosen action and any necessary details to perform it.\n",
        "\n",
        "4. **Evaluator:** This function checks if the result of the current subtask is reasonable and aligns with the overall goal. It returns an evaluation of the result and indicates whether the subtask needs to be retried.\n",
        "\n",
        "5. **generate_and_execute_code:** This function generates and executes Python code based on a given prompt and memory of previous steps. It returns both the generated code and its execution result.\n",
        "\n",
        "6. **executor:** Depending on the action decided by the \"actioner,\" this function either generates and executes code or returns reasoning. It handles the execution of tasks based on the action type.\n",
        "\n",
        "7. **final_answer_extractor:** After all subtasks are completed, this function gathers the results from previous steps to extract and provide the final answer to the user's query.\n",
        "\n",
        "8. **autonomous_agent:** This is the main function that coordinates the process of answering the user's query. It manages the entire sequence of planning, reasoning, action, evaluation, and final answer extraction to produce a complete response."
      ],
      "metadata": {
        "id": "goGb1KUVmVu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Web Scraper Function**"
      ],
      "metadata": {
        "id": "dFUYTqRdc1rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the web scraper function. I will store the response so that the planner can reference it.\n",
        "def web_scraper(url):\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "      soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "      title = soup.title.string if soup.title else \"No title found\"\n",
        "      meta_description = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
        "      description = meta_description[\"content\"] if meta_description else \"No description found\"\n",
        "\n",
        "      return {\"title\": title, \"description\": description}\n",
        "\n",
        "    else:\n",
        "\n",
        "      return {\"error\": f\"Failed to fetch the website. Status code: {response.status_code}\"}\n",
        "\n",
        "    scraped_data = web_scraper(url)\n",
        "    if \"error\" in scraped_data:\n",
        "        return scraped_data[\"error\"]\n",
        "\n",
        "    response = f\"The title of the page is '{scraped_data['title']}' and the description is '{scraped_data['description']}' \"\n",
        "    scraped_title = scraped_data['title']\n",
        "    scraped_data = scraped_data['description']\n",
        "    return response, scraped_title, scraped_data"
      ],
      "metadata": {
        "id": "ki48avL4dzUi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Planner Function**"
      ],
      "metadata": {
        "id": "8Kp2AXrxfxSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def planner(user_query: str) -> List[str]:\n",
        "   prompt = f\"\"\"Given the user's query: '{user_query}', break down the query into as few subtasks as possible in order to anser the question.\n",
        "   Each subtask is either a calculation or reasoning step. Never duplicate a task.\n",
        "\n",
        "   Each subtask can either:\n",
        "       - generate_code: This action involves generating Python code and executing it in order to make a calculation or verification.\n",
        "       - reasoning: This action involves providing reasoning for what to do to complete the subtask.\n",
        "\n",
        "   Each subtask should begin with either \"reasoning\" or \"generate_code\".\n",
        "\n",
        "   Keep in mind the overall goal of answering the user's query throughout the planning process.\n",
        "\n",
        "   Return the result as a JSON list of strings, where each string is a subtask.\n",
        "\n",
        "   Here is an example JSON response:\n",
        "\n",
        "   {{\n",
        "       \"subtasks\": [\"Subtask 1\", \"Subtask 2\", \"Subtask 3\"]\n",
        "   }}\n",
        "   \"\"\"\n",
        "    # I did not create another task for web scraping since the scraped_data also falls under the reasoning category, such as finding specific info that answers the user's prompt.\n",
        "   reasoning_prompt = \"\"\"\n",
        "   First, analyse and devise a step by step solution.\n",
        "   Given the web scraper's response: Title: '{scraped_title}' and  Description: '{scraped_description}',  use relevant information to complete a subtask.\n",
        "   Do not include any other sources of information.\n",
        "   Provide results relevant to the user's query.\n",
        "   \"\"\"\n",
        "   response = json.loads(get_llm_response(\"groq\", prompt, reasoning_prompt, json_mode=True)) # These are the parameters input to the LLM.\n",
        "   print(response)\n",
        "   return response[\"subtasks\"]"
      ],
      "metadata": {
        "id": "chuFNDKPdDAF"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reasoner Function**"
      ],
      "metadata": {
        "id": "FOfms1ndfrRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reasoner(user_query: str, subtasks: List[str], current_subtask: str, memory: List[Dict[str, Any]]) -> str:\n",
        "   prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}',\n",
        "\n",
        "   here are all the subtasks to complete in order to answer the user's query:\n",
        "   <subtasks>\n",
        "       {json.dumps(subtasks)}\n",
        "   </subtasks>\n",
        "\n",
        "   Here is the short-term memory (result of previous subtasks):\n",
        "   <memory>\n",
        "       {json.dumps(memory)}\n",
        "   </memory>\n",
        "\n",
        "   The current subtask to complete is:\n",
        "   <current_subtask>\n",
        "       {current_subtask}\n",
        "   </current_subtask>\n",
        "\n",
        "   - Provide concise reasoning on how to execute the current subtask, considering previous results where relevant.\n",
        "   - Prioritize explicit details over assumed patterns\n",
        "   - Avoid unnecessary complications in problem-solving.\n",
        "   - Keep the solution straightforward, without repeating subtasks.\n",
        "\n",
        "   Return the result as a JSON object with 'reasoning' as a key.\n",
        "\n",
        "   Example JSON response:\n",
        "   {{\n",
        "       \"reasoning\": \"2 sentences max on how to complete the current subtask.\"\n",
        "   }}\n",
        "   \"\"\"\n",
        "\n",
        "   response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "   return response[\"reasoning\"]"
      ],
      "metadata": {
        "id": "eYg6R7DtfvPg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Actioner Function**"
      ],
      "metadata": {
        "id": "PaNp_2pIiJMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def actioner(user_query: str, subtasks: List[str], current_subtask: str, reasoning: str, memory: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "   prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
        "\n",
        "   The subtasks are:\n",
        "   <subtasks>\n",
        "       {json.dumps(subtasks)}\n",
        "   </subtasks>\n",
        "\n",
        "   The current subtask is:\n",
        "   <current_subtask>\n",
        "       {current_subtask}\n",
        "   </current_subtask>\n",
        "\n",
        "   The reasoning for this subtask is:\n",
        "   <reasoning>\n",
        "       {reasoning}\n",
        "   </reasoning>\n",
        "\n",
        "   Here is the short-term memory (result of previous subtasks):\n",
        "   <memory>\n",
        "       {json.dumps(memory)}\n",
        "   </memory>\n",
        "\n",
        "   Determine the most appropriate action to take:\n",
        "       - If the task requires a calculation or verification through code, use the 'generate_code' action.\n",
        "       - If the task requires reasoning without code or calculation, use the 'reasoning' action.\n",
        "\n",
        "   Consider the overall goal and previous results when determining the action.\n",
        "\n",
        "   Return the result as a JSON object with 'action' and 'parameters' keys.  The 'parameters' key should always be a dictionary with 'prompt' as a key.\n",
        "\n",
        "   Example JSON responses:\n",
        "\n",
        "   {{\n",
        "       \"action\": \"generate_code\",\n",
        "       \"parameters\": {{\"prompt\": \"Write a function to calculate the area of a circle.\"}}\n",
        "   }}\n",
        "\n",
        "   {{\n",
        "       \"action\": \"reasoning\",\n",
        "       \"parameters\": {{\"prompt\": \"Explain how to complete the subtask.\"}}\n",
        "   }}\n",
        "   \"\"\"\n",
        "\n",
        "   response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "   return response"
      ],
      "metadata": {
        "id": "wBanUaasiJZ9"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluator Function**"
      ],
      "metadata": {
        "id": "i0OTYlazi65W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluator(user_query: str, subtasks: List[str], current_subtask: str, action_info: Dict[str, Any], execution_result: Dict[str, Any], memory: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "   prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
        "\n",
        "   The subtasks to complete to answer the user's query are:\n",
        "   <subtasks>\n",
        "       {json.dumps(subtasks)}\n",
        "   </subtasks>\n",
        "\n",
        "   The current subtask to complete is:\n",
        "   <current_subtask>\n",
        "       {current_subtask}\n",
        "   </current_subtask>\n",
        "\n",
        "   The result of the current subtask is:\n",
        "   <result>\n",
        "       {action_info}\n",
        "   </result>\n",
        "\n",
        "   The execution result of the current subtask is:\n",
        "   <execution_result>\n",
        "       {execution_result}\n",
        "   </execution_result>\n",
        "\n",
        "   Here is the short-term memory (result of previous subtasks):\n",
        "   <memory>\n",
        "       {json.dumps(memory)}\n",
        "   </memory>\n",
        "\n",
        "\n",
        "   Evaluate if the result is a reasonable answer for the current subtask, and makes sense in the context of the user's query.\n",
        "\n",
        "   Return a JSON object with 'evaluation' (string) and 'retry' (boolean) keys.\n",
        "\n",
        "   Example JSON response:\n",
        "   {{\n",
        "       \"evaluation\": \"The result is a reasonable answer for the current subtask.\",\n",
        "       \"retry\": false\n",
        "   }}\n",
        "   \"\"\"\n",
        "\n",
        "\n",
        "   response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "   return response\n"
      ],
      "metadata": {
        "id": "x-FByidSjEzZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extractor Function**"
      ],
      "metadata": {
        "id": "CCSdsHGjjOue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def final_answer_extractor(user_query: str, subtasks: List[str], memory: List[Dict[str, Any]]) -> str:\n",
        "   prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
        "\n",
        "   The subtasks completed to answer the user's query are:\n",
        "   <subtasks>\n",
        "       {json.dumps(subtasks)}\n",
        "   </subtasks>\n",
        "\n",
        "   The memory of the thought process (short-term memory) is:\n",
        "   <memory>\n",
        "       {json.dumps(memory)}\n",
        "   </memory>\n",
        "\n",
        "   Extract the final answer that addresses the user's query, from the memory.\n",
        "   Provide only the essential information without unnecessary explanations.\n",
        "\n",
        "   Return a JSON object with 'finalAnswer' as a key.\n",
        "\n",
        "   Here is an example JSON response:\n",
        "   {{\n",
        "       \"finalAnswer\": \"The final answer to the user's query, addressing all aspects of the question, based on the memory provided\",\n",
        "   }}\n",
        "   \"\"\"\n",
        "\n",
        "   response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "   return response[\"finalAnswer\"]\n"
      ],
      "metadata": {
        "id": "RCDk_cl6jSBt"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Generation Function**"
      ],
      "metadata": {
        "id": "ySTftiaWjZ6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_execute_code(prompt: str, user_query: str, memory: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "   code_generation_prompt = f\"\"\"\n",
        "\n",
        "   Only for a generate_code subtask:\n",
        "   - Generate Python code to implement the following task: '{prompt}'\n",
        "\n",
        "   - The overall goal is to answer the user's query: '{user_query}'\n",
        "\n",
        "   - Keep in mind the results of the previous subtasks, and use them to complete the current subtask.\n",
        "    <memory>\n",
        "        {json.dumps(memory)}\n",
        "    </memory>\n",
        "\n",
        "\n",
        "\n",
        "  -  Here are the guidelines for generating the code:\n",
        "        - Provide only the Python code, without explanations or markdown.\n",
        "        - The code must print or return a value.\n",
        "        - Don't include any backticks, code blocks or markdown like ```python or ``` in your response, just give me the code.\n",
        "        - Do not ever use the input() function in your code, use defined values instead.\n",
        "        - Do not ever import NLP libraries in your code, such as nltk, spacy, or any other NLP library.\n",
        "        - Generate straight-forward executable code, do not ever use functions.\n",
        "        - Don't ever provide the execution result in your response.\n",
        "        - Import all required libraries within the code itself.\n",
        "        - The code should be self-contained and ready to execute.\n",
        "        - Prioritize explicit details over assumed patterns.\n",
        "        - Keep the solution simple and efficient.\n",
        "    \"\"\"\n",
        "\n",
        "   generated_code = get_llm_response(\"groq\", code_generation_prompt)\n",
        "\n",
        "\n",
        "   print(f\"\\n\\nGenerated Code: start|{generated_code}|END\\n\\n\")\n",
        "\n",
        "   old_stdout = sys.stdout\n",
        "   sys.stdout = buffer = io.StringIO()\n",
        "\n",
        "   exec(generated_code)\n",
        "\n",
        "   sys.stdout = old_stdout\n",
        "   output = buffer.getvalue()\n",
        "\n",
        "   print(f\"\\n\\n***** Execution Result: |start|{output.strip()}|end| *****\\n\\n\")\n",
        "\n",
        "   return {\n",
        "       \"generated_code\": generated_code,\n",
        "       \"execution_result\": output.strip()\n",
        "   }\n"
      ],
      "metadata": {
        "id": "bwL2Kc6yjd4i"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Executor Function**"
      ],
      "metadata": {
        "id": "jzPZcH83lZrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def executor(action: str, parameters: Dict[str, Any], user_query: str, memory: List[Dict[str, Any]]) -> Any:\n",
        "   if action == \"generate_code\":\n",
        "       print(f\"Generating code for: {parameters['prompt']}\")\n",
        "       return generate_and_execute_code(parameters[\"prompt\"], user_query, memory)\n",
        "   elif action == \"reasoning\":\n",
        "       return parameters[\"prompt\"]\n",
        "   else:\n",
        "       return f\"Action '{action}' not implemented\""
      ],
      "metadata": {
        "id": "ugZF3jNnlc8-"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Autonomous Agent Function**"
      ],
      "metadata": {
        "id": "9dOo-_yzlg9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def autonomous_agent(user_query: str) -> List[Dict[str, Any]]:\n",
        "   memory = []\n",
        "   subtasks = planner(user_query)\n",
        "\n",
        "   print(\"User Query:\", user_query)\n",
        "   print(f\"Subtasks: {subtasks}\")\n",
        "\n",
        "   for subtask in subtasks:\n",
        "       max_retries = 1\n",
        "       for attempt in range(max_retries):\n",
        "\n",
        "           reasoning = reasoner(user_query, subtasks, subtask, memory)\n",
        "           action_info = actioner(user_query, subtasks, subtask, reasoning, memory)\n",
        "\n",
        "\n",
        "           print(f\"\\n\\n ****** Action Info: {action_info} ****** \\n\\n\")\n",
        "\n",
        "           execution_result = executor(action_info[\"action\"], action_info[\"parameters\"], user_query, memory)\n",
        "\n",
        "           print(f\"\\n\\n ****** Execution Result: {execution_result} ****** \\n\\n\")\n",
        "           evaluation = evaluator(user_query, subtasks, subtask, action_info, execution_result, memory)\n",
        "\n",
        "           step = {\n",
        "               \"subtask\": subtask,\n",
        "               \"reasoning\": reasoning,\n",
        "               \"action\": action_info,\n",
        "               \"evaluation\": evaluation\n",
        "           }\n",
        "           memory.append(step)\n",
        "\n",
        "           print(f\"\\n\\nSTEP: {step}\\n\\n\")\n",
        "\n",
        "           if not evaluation[\"retry\"]:\n",
        "               break\n",
        "\n",
        "           if attempt == max_retries - 1:\n",
        "               print(f\"Max retries reached for subtask: {subtask}\")\n",
        "\n",
        "   final_answer = final_answer_extractor(user_query, subtasks, memory)\n",
        "   return final_answer"
      ],
      "metadata": {
        "id": "zXScHD-UllZW"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Running all the code at once**"
      ],
      "metadata": {
        "id": "4UHxaYist7oB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import sys\n",
        "import io\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Web Scraper function\n",
        "def web_scraper(url: str) -> dict:\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        title = soup.title.string if soup.title else \"No title found\"\n",
        "        meta_description = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
        "        description = meta_description[\"content\"] if meta_description else \"No description found\"\n",
        "        return {\"title\": title, \"description\": description}\n",
        "    else:\n",
        "        return {\"error\": f\"Failed to fetch the website. Status code: {response.status_code}\"}\n",
        "\n",
        "# Planner function\n",
        "def planner(user_query: str, scraped_title: str, scraped_description: str) -> List[str]:\n",
        "    prompt = f\"\"\"Given the user's query: '{user_query}', break down the query into as few subtasks as possible in order to answer the question.\n",
        "    Each subtask is either a calculation or reasoning step. Never duplicate a task.\n",
        "\n",
        "    Each subtask can either:\n",
        "        - generate_code: This action involves generating Python code and executing it in order to make a calculation or verification.\n",
        "        - reasoning: This action involves providing reasoning for what to do to complete the subtask.\n",
        "\n",
        "    Each subtask should begin with either \"reasoning\" or \"generate_code\".\n",
        "\n",
        "    Keep in mind the overall goal of answering the user's query throughout the planning process.\n",
        "\n",
        "    Return the result as a JSON list of strings, where each string is a subtask.\n",
        "    \"\"\"\n",
        "\n",
        "    reasoning_prompt = f\"\"\"\n",
        "    First, analyze and devise a step-by-step solution.\n",
        "    Given the web scraper's response: Title: '{scraped_title}' and Description: '{scraped_description}', use relevant information to complete a subtask.\n",
        "    Provide results relevant to the user's query.\n",
        "    \"\"\"\n",
        "\n",
        "    # Simulated response from LLM\n",
        "    response = json.loads(get_llm_response(\"groq\", prompt, reasoning_prompt, json_mode=True))\n",
        "    return response[\"subtasks\"]\n",
        "\n",
        "# Reasoner Function\n",
        "def reasoner(user_query: str, subtasks: List[str], current_subtask: str, memory: List[Dict[str, Any]]) -> str:\n",
        "    prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}',\n",
        "    here are all the subtasks to complete in order to answer the user's query:\n",
        "    <subtasks>\n",
        "        {json.dumps(subtasks)}\n",
        "    </subtasks>\n",
        "\n",
        "    Here is the short-term memory (result of previous subtasks):\n",
        "    <memory>\n",
        "        {json.dumps(memory)}\n",
        "    </memory>\n",
        "\n",
        "    The current subtask to complete is:\n",
        "    <current_subtask>\n",
        "        {current_subtask}\n",
        "    </current_subtask>\n",
        "\n",
        "    - Provide concise reasoning on how to execute the current subtask, considering previous results where relevant.\n",
        "    - Prioritize explicit details over assumed patterns\n",
        "    - Avoid unnecessary complications in problem-solving.\n",
        "    - Keep the solution straightforward, without repeating subtasks.\n",
        "\n",
        "    Return the result as a JSON object with 'reasoning' as a key.\n",
        "    \"\"\"\n",
        "    response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "    return response[\"reasoning\"]\n",
        "\n",
        "# Actioner Function\n",
        "# Actioner Function\n",
        "def actioner(user_query: str, subtasks: List[str], current_subtask: str, reasoning: str, memory: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
        "\n",
        "    The subtasks are:\n",
        "    <subtasks>\n",
        "        {json.dumps(subtasks)}\n",
        "    </subtasks>\n",
        "\n",
        "    The current subtask is:\n",
        "    <current_subtask>\n",
        "        {current_subtask}\n",
        "    </current_subtask>\n",
        "\n",
        "    The reasoning for this subtask is:\n",
        "    <reasoning>\n",
        "        {reasoning}\n",
        "    </reasoning>\n",
        "\n",
        "    Here is the short-term memory (result of previous subtasks):\n",
        "    <memory>\n",
        "        {json.dumps(memory)}\n",
        "    </memory>\n",
        "\n",
        "    Determine the most appropriate action to take for each subtask.:\n",
        "        - If the task requires a calculation or verification through code, use the 'generate_code' action.\n",
        "        - If the task requires a response without code or calculation, use the 'reasoning' action.\n",
        "\n",
        "    Consider the overall goal and previous results when determining the action.\n",
        "\n",
        "    Return the result as a JSON object with 'action' and 'parameters' keys. The 'parameters' key should always be a dictionary with 'prompt' as a key.\n",
        "\n",
        "    Example JSON responses:\n",
        "    {{\n",
        "        \"action\": \"generate_code\",\n",
        "        \"parameters\": {{\"prompt\": \"Write a function to calculate the area of a circle.\"}}\n",
        "    }}\n",
        "\n",
        "    {{\n",
        "        \"action\": \"reasoning\",\n",
        "        \"parameters\": {{\"prompt\": \"Explain how to complete the subtask.\"}}\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "\n",
        "    # Check if 'action' and 'parameters' keys exist in response\n",
        "    if 'action' not in response:\n",
        "        print(f\"Warning: 'action' key not found in response from LLM: {response}\")\n",
        "        response['action'] = 'reasoning'  # Default action if not found\n",
        "\n",
        "    if 'parameters' not in response:\n",
        "        print(f\"Warning: 'parameters' key not found in response from LLM: {response}\")\n",
        "        response['parameters'] = {'prompt': 'Provide reasoning for the current subtask.'}  # Default parameters\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "# Evaluator Function\n",
        "def evaluator(user_query: str, subtasks: List[str], current_subtask: str, action_info: Dict[str, Any], execution_result: Dict[str, Any], memory: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
        "    The subtasks to complete to answer the user's query are:\n",
        "    <subtasks>\n",
        "        {json.dumps(subtasks)}\n",
        "    </subtasks>\n",
        "\n",
        "    The current subtask to complete is:\n",
        "    <current_subtask>\n",
        "        {current_subtask}\n",
        "    </current_subtask>\n",
        "\n",
        "    The result of the current subtask is:\n",
        "    <result>\n",
        "        {action_info}\n",
        "    </result>\n",
        "\n",
        "    The execution result of the current subtask is:\n",
        "    <execution_result>\n",
        "        {execution_result}\n",
        "    </execution_result>\n",
        "\n",
        "    Here is the short-term memory (result of previous subtasks):\n",
        "    <memory>\n",
        "        {json.dumps(memory)}\n",
        "    </memory>\n",
        "\n",
        "    Evaluate if the result is a reasonable answer for the current subtask, and makes sense in the context of the user's query.\n",
        "\n",
        "    Return a JSON object with 'evaluation' (string) and 'retry' (boolean) keys.\n",
        "    \"\"\"\n",
        "    response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "    return response\n",
        "\n",
        "# Extracting the final answer\n",
        "def final_answer_extractor(user_query: str, subtasks: List[str], memory: List[Dict[str, Any]]) -> str:\n",
        "    prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
        "    The subtasks completed to answer the user's query are:\n",
        "    <subtasks>\n",
        "        {json.dumps(subtasks)}\n",
        "    </subtasks>\n",
        "\n",
        "    The memory of the thought process (short-term memory) is:\n",
        "    <memory>\n",
        "        {json.dumps(memory)}\n",
        "    </memory>\n",
        "\n",
        "    Extract the final answer that addresses the user's query, from the memory.\n",
        "    Provide only the essential information without unnecessary explanations.\n",
        "\n",
        "    Return a JSON object with 'finalAnswer' as a key.\n",
        "    \"\"\"\n",
        "    response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "    return response[\"finalAnswer\"]\n",
        "\n",
        "# Code Generator and Executor Function\n",
        "def generate_and_execute_code(prompt: str, user_query: str, memory: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    code_generation_prompt = f\"\"\"\n",
        "    Only for a generate_code subtask:\n",
        "    - Generate Python code to implement the following task: '{prompt}'\n",
        "    - The overall goal is to answer the user's query: '{user_query}'\n",
        "    - Keep in mind the results of the previous subtasks, and use them to complete the current subtask.\n",
        "    <memory>\n",
        "        {json.dumps(memory)}\n",
        "    </memory>\n",
        "\n",
        "    Here are the guidelines for generating the code:\n",
        "        - Provide only the Python code, without explanations or markdown.\n",
        "        - The code must print or return a value.\n",
        "        - Don't include any backticks, code blocks or markdown like ```python or ``` in your response, just give me the code.\n",
        "        - Do not ever use the input() function in your code, use defined values instead.\n",
        "        - Do not ever import NLP libraries in your code, such as nltk, spacy, or any other NLP library.\n",
        "        - Generate straight-forward executable code, do not ever use functions.\n",
        "        - Don't ever provide the execution result in your response.\n",
        "        - Import all required libraries within the code itself.\n",
        "        - The code should be self-contained and ready to execute.\n",
        "        - Prioritize explicit details over assumed patterns.\n",
        "        - Keep the solution simple and efficient.\n",
        "    \"\"\"\n",
        "\n",
        "    generated_code = get_llm_response(\"groq\", code_generation_prompt)\n",
        "\n",
        "    print(f\"\\n\\nGenerated Code: start|{generated_code}|END\\n\\n\")\n",
        "\n",
        "    old_stdout = sys.stdout\n",
        "    sys.stdout = buffer = io.StringIO()\n",
        "\n",
        "    exec(generated_code)\n",
        "\n",
        "    sys.stdout = old_stdout\n",
        "    output = buffer.getvalue()\n",
        "\n",
        "    print(f\"\\n\\n***** Execution Result: |start|{output.strip()}|end| *****\\n\\n\")\n",
        "\n",
        "    return {\n",
        "        \"generated_code\": generated_code,\n",
        "        \"execution_result\": output.strip()\n",
        "    }\n",
        "\n",
        "# Executor function\n",
        "def executor(action: str, parameters: Dict[str, Any], user_query: str, memory: List[Dict[str, Any]]) -> Any:\n",
        "    if action == \"generate_code\":\n",
        "        print(f\"Generating code for: {parameters.get('prompt', 'No prompt provided')}\")\n",
        "        return generate_and_execute_code(parameters.get(\"prompt\", ''), user_query, memory)\n",
        "    elif action == \"reasoning\":\n",
        "        return parameters.get(\"prompt\", 'No reasoning prompt provided')\n",
        "    else:\n",
        "        return f\"Action '{action}' not implemented\"\n",
        "\n",
        "\n",
        "# Autonomous Agent\n",
        "def autonomous_agent(user_query: str) -> List[Dict[str, Any]]:\n",
        "    memory = []\n",
        "\n",
        "    # Web scraping\n",
        "    url = \"https://blog.samaltman.com/what-i-wish-someone-had-told-me\"\n",
        "    scraped_data = web_scraper(url)\n",
        "\n",
        "    if \"error\" in scraped_data:\n",
        "        print(scraped_data[\"error\"])  # Handle the error\n",
        "        return []\n",
        "\n",
        "    scraped_title = scraped_data['title']\n",
        "    scraped_description = scraped_data['description']\n",
        "\n",
        "    # Planning\n",
        "    subtasks = planner(user_query, scraped_title, scraped_description)\n",
        "\n",
        "    # Iterating through each subtask\n",
        "    for subtask in subtasks:\n",
        "        current_subtask = subtask\n",
        "        reasoning = reasoner(user_query, subtasks, current_subtask, memory)\n",
        "        action_info = actioner(user_query, subtasks, current_subtask, reasoning, memory)\n",
        "\n",
        "        execution_result = executor(action_info['action'], action_info['parameters'], user_query, memory)\n",
        "\n",
        "        # Evaluating\n",
        "        evaluation_result = evaluator(user_query, subtasks, current_subtask, action_info, execution_result, memory)\n",
        "\n",
        "        if evaluation_result['retry']:\n",
        "            print(f\"Retrying subtask: {current_subtask}\")\n",
        "            # Logic for retrying if necessary (not implemented in this example)\n",
        "\n",
        "        # Append to memory\n",
        "        memory.append({\n",
        "            \"subtask\": current_subtask,\n",
        "            \"reasoning\": reasoning,\n",
        "            \"action_info\": action_info,\n",
        "            \"execution_result\": execution_result,\n",
        "            \"evaluation_result\": evaluation_result\n",
        "        })\n",
        "\n",
        "    # Extracting final answer\n",
        "    final_answer = final_answer_extractor(user_query, subtasks, memory)\n",
        "    return final_answer\n",
        "\n",
        "# Testing the agent\n",
        "user_query = \"What does Sam Altman say in the blog What I Wish Someone Had Told Me?\"\n",
        "final_result = autonomous_agent(user_query)\n",
        "final_result\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "DVZAVo82v-2v",
        "outputId": "fc6052c6-de1e-46a1-d1c3-99ac570a7af4"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sam Altman suggests qualities of an effective mentor that one wishes someone had told them when they were starting their journey, such as seeking mentors from a diverse set, avoiding common pitfalls, and maintaining a growth mindset.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI o1-preview model getting trivial questions wrong\n",
        "\n",
        "- [Link to Reddit post](https://www.reddit.com/r/ChatGPT/comments/1ff9w7y/new_o1_still_fails_miserably_at_trivial_questions/)\n",
        "- Links to ChatGPT threads: [(1)](https://chatgpt.com/share/66f21757-db2c-8012-8b0a-11224aed0c29), [(2)](https://chatgpt.com/share/66e3c1e5-ae00-8007-8820-fee9eb61eae5)\n",
        "- [Improving reasoning in LLMs through thoughtful prompting](https://www.reddit.com/r/singularity/comments/1fdhs2m/did_i_just_fix_the_data_overfitting_problem_in/?share_id=6DsDLJUu1qEx_bsqFDC8a&utm_content=2&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1)\n"
      ],
      "metadata": {
        "id": "MrSeaoPqq1qY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing the autonomous agent**"
      ],
      "metadata": {
        "id": "y7CIUPSflzTX"
      }
    }
  ]
}