{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![Img](https://app.theheadstarter.com/static/hs-logo-opengraph.png)\n",
        "\n",
        "# Headstarter AI Agent Workshop\n",
        "\n",
        "#### **Skills: OpenAI, Groq, Llama, OpenRouter**\n",
        "\n",
        "## **To Get Started:**\n",
        "1. [Get your Groq API Key](https://console.groq.com/keys)\n",
        "2. [Get your OpenRouter API Key](https://openrouter.ai/settings/keys)\n",
        "3. [Get your OpenAI API Key](https://platform.openai.com/api-keys)\n",
        "\n",
        "\n",
        "### **Interesting Reads**\n",
        "- [Sam Altman's Blog Post: The Intelligence Age](https://ia.samaltman.com/)\n",
        "- [What LLMs cannot do](https://ehudreiter.com/2023/12/11/what-llms-cannot-do/)\n",
        "- [Chain of Thought Prompting](https://www.promptingguide.ai/techniques/cot)\n",
        "- [Why ChatGPT can't count the number of r's in the word strawberry](https://prompt.16x.engineer/blog/why-chatgpt-cant-count-rs-in-strawberry)\n",
        "\n",
        "\n",
        "## During the Workshop\n",
        "- [Any code shared during the workshop will be posted here](https://docs.google.com/document/d/1hPBJt_4Ihkj6v667fWxVjzwCMS4uBPdYlBLd2IqkxJ0/edit?usp=sharing)"
      ],
      "metadata": {
        "id": "nO8gHDbSAa4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install necessary libraries"
      ],
      "metadata": {
        "id": "Pt_SMGlvocqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install openai groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKvKwO8XAnPt",
        "outputId": "a32ad6b4-aa70-4fc3-bdcb-3f71437a7792"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.49.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting groq\n",
            "  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Downloading openai-1.49.0-py3-none-any.whl (378 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.1/378.1 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groq-0.11.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, openai, groq\n",
            "Successfully installed groq-0.11.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 openai-1.49.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up Groq, OpenRouter, & OpenAI clients"
      ],
      "metadata": {
        "id": "GjcgEeFaof1i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pHbjDU_L__Vd"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import json\n",
        "from groq import Groq\n",
        "import json\n",
        "from typing import List, Dict, Any, Callable\n",
        "import ast\n",
        "import io\n",
        "import sys\n",
        "\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "os.environ['GROQ_API_KEY'] = groq_api_key\n",
        "\n",
        "openrouter_api_key = userdata.get(\"OPENROUTER_API_KEY\")\n",
        "os.environ['OPENROUTER_API_KEY'] = openrouter_api_key\n",
        "\n",
        "openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "groq_client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
        "\n",
        "openrouter_client = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=os.getenv(\"OPENROUTER_API_KEY\")\n",
        ")\n",
        "\n",
        "openai_client = OpenAI(\n",
        "    base_url=\"https://api.openai.com/v1\",\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define functions to easily query and compare responses from OpenAI, Groq, and OpenRouter"
      ],
      "metadata": {
        "id": "AZF6uLpooj-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_llm_response(client, prompt, openai_model=\"o1-preview\", json_mode=False):\n",
        "\n",
        "    if client == \"openai\":\n",
        "\n",
        "        kwargs = {\n",
        "            \"model\": openai_model,\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "        }\n",
        "\n",
        "        if json_mode:\n",
        "            kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
        "\n",
        "        response = openai_client.chat.completions.create(**kwargs)\n",
        "\n",
        "    elif client == \"groq\":\n",
        "\n",
        "        try:\n",
        "            models = [\"llama-3.1-8b-instant\", \"llama-3.1-70b-versatile\", \"llama3-70b-8192\", \"llama3-8b-8192\", \"gemma2-9b-it\"]\n",
        "\n",
        "            for model in models:\n",
        "\n",
        "                try:\n",
        "                    kwargs = {\n",
        "                        \"model\": model,\n",
        "                        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "                    }\n",
        "                    if json_mode:\n",
        "                        kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
        "\n",
        "                    response = groq_client.chat.completions.create(**kwargs)\n",
        "\n",
        "                    break\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error: {e}\")\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "            kwargs = {\n",
        "                \"model\": \"meta-llama/llama-3.1-8b-instruct:free\",\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "            }\n",
        "\n",
        "            if json_mode:\n",
        "                kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
        "\n",
        "            response = openrouter_client.chat.completions.create(**kwargs)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid client: {client}\")\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "def evaluate_responses(prompt, reasoning_prompt=False, openai_model=\"o1-preview\"):\n",
        "\n",
        "    if reasoning_prompt:\n",
        "        prompt = f\"{prompt}\\n\\n{reasoning_prompt}.\"\n",
        "\n",
        "    openai_response = get_llm_response(\"openai\", prompt, openai_model)\n",
        "    groq_response = get_llm_response(\"groq\", prompt)\n",
        "\n",
        "    print(f\"OpenAI Response: {openai_response}\")\n",
        "    print(f\"\\n\\nGroq Response: {groq_response}\")"
      ],
      "metadata": {
        "id": "BNYyDCNuAhbj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt1 = \"How many r's are in the word 'strawberry'?\"\n",
        "evaluate_responses(prompt1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9V03RXNBd6j",
        "outputId": "0aa64363-0e4d-4b22-94be-aecdf0e384e6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI Response: There are three \"r\"s in the word \"strawberry\".\n",
            "\n",
            "\n",
            "Groq Response: In the word 'strawberry', there are 2 'R's.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_responses(prompt1, openai_model=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qM8-nKzGpaKj",
        "outputId": "b0050740-501d-45b1-b422-c6c5b73c94b9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI Response: There are two r's in the word 'strawberry'.\n",
            "\n",
            "\n",
            "Groq Response: There are 2 'r's in the word 'strawberry'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_responses(prompt1, openai_model=\"gpt-4o-mini\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9-Rtjyhpmn3",
        "outputId": "380be035-3a65-4812-e191-b26a4d19b8eb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI Response: The word \"strawberry\" contains 2 letters 'r'.\n",
            "\n",
            "\n",
            "Groq Response: There are 2 r's in the word 'strawberry'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt2 = \"9.9 or 9.11, which number is bigger?\"\n",
        "evaluate_responses(prompt2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYaiRrHwbwMT",
        "outputId": "fc9376f3-9255-43d7-8bca-1ee25a46e09b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI Response: To determine which number is larger between 9.9 and 9.11, we'll compare them digit by digit.\n",
            "\n",
            "First, both numbers have the same whole number part: **9**.\n",
            "\n",
            "Next, we'll compare the decimal parts:\n",
            "\n",
            "- **9.9** can be thought of as **9.90** for easier comparison.\n",
            "- **9.11** remains as **9.11**.\n",
            "\n",
            "Now, compare the first digit after the decimal:\n",
            "\n",
            "- **9** in **9.90**\n",
            "- **1** in **9.11**\n",
            "\n",
            "Since **9 > 1**, it's clear that **9.9** is larger than **9.11**.\n",
            "\n",
            "**Answer:** 9.9 is larger than 9.11; thus, 9.9 is the bigger number.\n",
            "\n",
            "\n",
            "Groq Response: 9.11 is bigger than 9.9. It's quite straightforward in a simple decimal comparison.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reasoning_prompt = \"Let's first understand the problem and devise a plan to solve it. Then, let's carry out the plan and solve the problem step by step.\"\n",
        "evaluate_responses(prompt1, reasoning_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mq1YkfFeCRl9",
        "outputId": "a00b0a3a-2d7f-4f6b-aaaf-cd3f19b666af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI Response: Certainly! Let's solve the problem step by step.\n",
            "\n",
            "---\n",
            "\n",
            "**1. Understand the Problem**\n",
            "\n",
            "We are asked to find out how many times the letter **'r'** appears in the word **'strawberry'**.\n",
            "\n",
            "**2. Devise a Plan**\n",
            "\n",
            "- Write down the word clearly.\n",
            "- Examine each letter in the word one by one.\n",
            "- Count and keep track of every time the letter 'r' appears.\n",
            "\n",
            "**3. Carry Out the Plan**\n",
            "\n",
            "Let's write down the word and analyze each letter:\n",
            "\n",
            "**Word:** **S T R A W B E R R Y**\n",
            "\n",
            "Now, let's go through each letter:\n",
            "\n",
            "1. **S** - Not 'r' (Count = 0)\n",
            "2. **T** - Not 'r' (Count = 0)\n",
            "3. **R** - This is 'r' (Count = 1)\n",
            "4. **A** - Not 'r' (Count = 1)\n",
            "5. **W** - Not 'r' (Count = 1)\n",
            "6. **B** - Not 'r' (Count = 1)\n",
            "7. **E** - Not 'r' (Count = 1)\n",
            "8. **R** - This is 'r' (Count = 2)\n",
            "9. **R** - This is 'r' (Count = 3)\n",
            "10. **Y** - Not 'r' (Count = 3)\n",
            "\n",
            "**4. Answer**\n",
            "\n",
            "After examining each letter, we have found that the letter **'r'** appears **3 times** in the word **'strawberry'**.\n",
            "\n",
            "---\n",
            "\n",
            "**Conclusion:** There are **3** 'r's in the word **'strawberry'**.\n",
            "\n",
            "\n",
            "Groq Response: To solve this problem, we need to understand that we are looking for the count of the letter 'r' in the word 'strawberry'.\n",
            "\n",
            "Step 1: Identify the word and the target letter.\n",
            "- The word is 'strawberry'.\n",
            "- The target letter is 'r'.\n",
            "\n",
            "Step 2: Write down the word and visually scan for 'r's.\n",
            "strawberr(y)\n",
            "\n",
            "Step 3: Start visually scanning from the beginning and count the 'r's.\n",
            "In the word 'strawberry', the first 'r' appears in the word after the \"b\" which is after the letters 'berr'. The second 'r' also appears after that 'berr' so we notice two.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reasoning_prompt = \"\"\"Let's first understand the problem and devise a plan to solve it. Verify the plan step by step. Then, let's carry out the plan and solve the problem step by step. Lastly, let's verify the answer step by step by working backwards. Determine if the answer is correct or not, and if not, reflect on on why it is incorrect. Then, think step by step about how to arrive at the correct answer.\"\"\"\n",
        "evaluate_responses(prompt1, reasoning_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgyXx8M9C0RT",
        "outputId": "56053925-c8fb-4fe9-ea4d-c74deab56cbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI Response: **Understanding the Problem:**\n",
            "\n",
            "We need to determine **how many times the letter 'r' appears** in the word **'strawberry'**.\n",
            "\n",
            "---\n",
            "\n",
            "**Devise a Plan to Solve It:**\n",
            "\n",
            "1. **Write down** the word 'strawberry'.\n",
            "2. **List each letter** in the word sequentially.\n",
            "3. **Identify** every occurrence of the letter 'r'.\n",
            "4. **Count** the total number of times 'r' appears.\n",
            "\n",
            "---\n",
            "\n",
            "**Verify the Plan Step by Step:**\n",
            "\n",
            "- **Step 1 and 2** will help us see all letters clearly.\n",
            "- **Step 3** ensures we find all 'r's without missing any.\n",
            "- **Step 4** gives us the total count required.\n",
            "\n",
            "---\n",
            "\n",
            "**Carry Out the Plan and Solve the Problem Step by Step:**\n",
            "\n",
            "1. **Write down the word:**\n",
            "\n",
            "   ```\n",
            "   S T R A W B E R R Y\n",
            "   ```\n",
            "\n",
            "2. **List each letter with its position:**\n",
            "\n",
            "   - Position 1: **S**\n",
            "   - Position 2: **T**\n",
            "   - Position 3: **R**\n",
            "   - Position 4: **A**\n",
            "   - Position 5: **W**\n",
            "   - Position 6: **B**\n",
            "   - Position 7: **E**\n",
            "   - Position 8: **R**\n",
            "   - Position 9: **R**\n",
            "   - Position 10: **Y**\n",
            "\n",
            "3. **Identify every 'r':**\n",
            "\n",
            "   - **R** at Position 3\n",
            "   - **R** at Position 8\n",
            "   - **R** at Position 9\n",
            "\n",
            "4. **Count the total 'r's:**\n",
            "\n",
            "   - There are **3** occurrences of the letter 'r'.\n",
            "\n",
            "---\n",
            "\n",
            "**Verify the Answer Step by Step by Working Backwards:**\n",
            "\n",
            "- Starting with the total count **3**, we confirm each occurrence:\n",
            "  - First **'r'** at Position 3\n",
            "  - Second **'r'** at Position 8\n",
            "  - Third **'r'** at Position 9\n",
            "- We ensure no other 'r's are present in the word.\n",
            "\n",
            "---\n",
            "\n",
            "**Determine if the Answer is Correct or Not:**\n",
            "\n",
            "- **The answer is correct.**\n",
            "- There are **3** 'r's in 'strawberry'.\n",
            "\n",
            "---\n",
            "\n",
            "**Reflection:**\n",
            "\n",
            "If we initially thought there were fewer 'r's, we might have:\n",
            "\n",
            "- **Overlooked** the consecutive 'r's at Positions 8 and 9.\n",
            "- **Miscounted** by not listing all letters systematically.\n",
            "\n",
            "By carefully listing and counting, we've arrived at the correct answer.\n",
            "\n",
            "---\n",
            "\n",
            "**Final Answer:**\n",
            "\n",
            "There are **3** 'r's in the word **'strawberry'**.\n",
            "\n",
            "\n",
            "Groq Response: **Step 1: Understand the problem**\n",
            "\n",
            "The problem asks us to count the number of 'r's in the word \"strawberry\".\n",
            "\n",
            "**Step 2: Devise a plan to solve it**\n",
            "\n",
            "Our plan is to:\n",
            "1. Write down the word \"strawberry\" to clearly see its letters.\n",
            "2. Locate each occurrence of the letter 'r' in the word.\n",
            "3. Count the number of 'r's found.\n",
            "\n",
            "**Step 3: Carry out the plan**\n",
            "\n",
            "The word \"strawberry\" is written as \"S-T-R-A-W-B-E-R-R-Y\".\n",
            "\n",
            "Now, we locate each occurrence of the letter 'r': we see 'r' in positions 5, 6, and 7.\n",
            "\n",
            "**Step 4: Count the 'r's**\n",
            "\n",
            "There are 3 occurrences of the letter 'r', so we can conclude that there are 3 'r's in the word \"strawberry\".\n",
            "\n",
            "**Step 5: Verify the answer step by step**\n",
            "\n",
            "To verify our answer, let's work backwards:\n",
            "\n",
            "- The last step was counting the 'r's and finding that there are 3.\n",
            "- Before that, we were observing the positions of 'r's in the word, which showed 3 'r's.\n",
            "- Before observing the positions, we had written down the word, listing all its letters.\n",
            "- Writing down the word \"strawberry\" correctly would lead us to correctly observe and count its 'r's.\n",
            "\n",
            "Our answer is verified to be correct.\n",
            "\n",
            "**Conclusion**\n",
            "\n",
            "There are 3 'r's in the word \"strawberry\", and our answer is accurate following our plan and verification steps.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent Architecture\n",
        "\n",
        "[![](https://mermaid.ink/img/pako:eNqFUslugzAQ_ZWRz8kPILUVCZClVdQmqdTK5ODCFFDAjry0iSD_XmOTJodK5WS_Zd7M4JZkIkcSkEKyQwnbKOVgvzf6qlDCi0F52sF4fA-hJ0IaGi24aIRREBbItacn9LlmnKPced2kR7s1aiO5AmU-NFN71cG0fRLiALqUwhQlbAbi7F1TVyuia2RKXItFDo5pmOnqBo5dhgcDmKFlmEaY2oE6SOgvwHgO8REzM5B_2n1kxYsOZtSrxSUocfkzf5m5y5zGX6w27Cqau3IDOpTU8gR3kLBa2Y4W7QqP-jLywzDywtnesd_NLbISHSxpUnFWQ8jVt_0b8VFLll0Tl66TR-q3DLfaf3vaSoMukYxIg7JhVW4fQdvbUqJLbDAlgT3mTO5TkvKz1TG7ks2JZyTQ1j0i5pDb9UYVs2-nIcFnP-YFjfPKNjqA5x9CM8YW?type=png)](https://mermaid.live/edit#pako:eNqFUslugzAQ_ZWRz8kPILUVCZClVdQmqdTK5ODCFFDAjry0iSD_XmOTJodK5WS_Zd7M4JZkIkcSkEKyQwnbKOVgvzf6qlDCi0F52sF4fA-hJ0IaGi24aIRREBbItacn9LlmnKPced2kR7s1aiO5AmU-NFN71cG0fRLiALqUwhQlbAbi7F1TVyuia2RKXItFDo5pmOnqBo5dhgcDmKFlmEaY2oE6SOgvwHgO8REzM5B_2n1kxYsOZtSrxSUocfkzf5m5y5zGX6w27Cqau3IDOpTU8gR3kLBa2Y4W7QqP-jLywzDywtnesd_NLbISHSxpUnFWQ8jVt_0b8VFLll0Tl66TR-q3DLfaf3vaSoMukYxIg7JhVW4fQdvbUqJLbDAlgT3mTO5TkvKz1TG7ks2JZyTQ1j0i5pDb9UYVs2-nIcFnP-YFjfPKNjqA5x9CM8YW)\n",
        "\n",
        "\n",
        "![agent_architecture_v2](https://github.com/user-attachments/assets/a65b6db9-bef1-4579-aed3-01444ce40544)"
      ],
      "metadata": {
        "id": "RxpUp2KED9hh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To create our AI Agent, we will define the following functions:\n",
        "\n",
        "1. **Planner:** This function takes a user's query and breaks it down into smaller, manageable subtasks. It returns these subtasks as a list, where each one is either a reasoning task or a code generation task.\n",
        "\n",
        "2. **Reasoner:** This function provides reasoning on how to complete a specific subtask, considering both the overall query and the results of any previous subtasks. It returns a short explanation on how to proceed with the current subtask.\n",
        "\n",
        "3. **Actioner:** Based on the reasoning provided for a subtask, this function decides whether the next step requires generating code or more reasoning. It then returns the chosen action and any necessary details to perform it.\n",
        "\n",
        "4. **Evaluator:** This function checks if the result of the current subtask is reasonable and aligns with the overall goal. It returns an evaluation of the result and indicates whether the subtask needs to be retried.\n",
        "\n",
        "5. **generate_and_execute_code:** This function generates and executes Python code based on a given prompt and memory of previous steps. It returns both the generated code and its execution result.\n",
        "\n",
        "6. **executor:** Depending on the action decided by the \"actioner,\" this function either generates and executes code or returns reasoning. It handles the execution of tasks based on the action type.\n",
        "\n",
        "7. **final_answer_extractor:** After all subtasks are completed, this function gathers the results from previous steps to extract and provide the final answer to the user's query.\n",
        "\n",
        "8. **autonomous_agent:** This is the main function that coordinates the process of answering the user's query. It manages the entire sequence of planning, reasoning, action, evaluation, and final answer extraction to produce a complete response."
      ],
      "metadata": {
        "id": "goGb1KUVmVu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def planner(user_query: str) -> List[str]:\n",
        "    prompt = f\"\"\"Given the user's query: '{user_query}', break down the query into as few subtasks as possible in order to anser the question.\n",
        "    Each subtask is either a calculation or reasoning step. Never duplicate a task.\n",
        "\n",
        "    Here are the only 2 actions that can be taken for each subtask:\n",
        "        - generate_code: This action involves generating Python code and executing it in order to make a calculation or verification.\n",
        "        - reasoning: This action involves providing reasoning for what to do to complete the subtask.\n",
        "\n",
        "    Each subtask should begin with either \"reasoning\" or \"generate_code\".\n",
        "\n",
        "    Keep in mind the overall goal of answering the user's query throughout the planning process.\n",
        "\n",
        "    Return the result as a JSON list of strings, where each string is a subtask.\n",
        "\n",
        "    Here is an example JSON response:\n",
        "\n",
        "    {{\n",
        "        \"subtasks\": [\"Subtask 1\", \"Subtask 2\", \"Subtask 3\"]\n",
        "    }}\n",
        "    \"\"\"\n",
        "    response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "    print(response)\n",
        "    return response[\"subtasks\"]\n",
        "\n",
        "def reasoner(user_query: str, subtasks: List[str], current_subtask: str, memory: List[Dict[str, Any]]) -> str:\n",
        "    prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
        "\n",
        "    Here are all the subtasks to complete in order to answer the user's query:\n",
        "    <subtasks>\n",
        "        {json.dumps(subtasks)}\n",
        "    </subtasks>\n",
        "\n",
        "    Here is the short-term memory (result of previous subtasks):\n",
        "    <memory>\n",
        "        {json.dumps(memory)}\n",
        "    </memory>\n",
        "\n",
        "    The current subtask to complete is:\n",
        "    <current_subtask>\n",
        "        {current_subtask}\n",
        "    </current_subtask>\n",
        "\n",
        "    - Provide concise reasoning on how to execute the current subtask, considering previous results.\n",
        "    - Prioritize explicit details over assumed patterns\n",
        "    - Avoid unnecessary complications in problem-solving\n",
        "\n",
        "    Return the result as a JSON object with 'reasoning' as a key.\n",
        "\n",
        "    Example JSON response:\n",
        "    {{\n",
        "        \"reasoning\": \"2 sentences max on how to complete the current subtask.\"\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "    return response[\"reasoning\"]\n",
        "\n",
        "def actioner(user_query: str, subtasks: List[str], current_subtask: str, reasoning: str, memory: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
        "\n",
        "    The subtasks are:\n",
        "    <subtasks>\n",
        "        {json.dumps(subtasks)}\n",
        "    </subtasks>\n",
        "\n",
        "    The current subtask is:\n",
        "    <current_subtask>\n",
        "        {current_subtask}\n",
        "    </current_subtask>\n",
        "\n",
        "    The reasoning for this subtask is:\n",
        "    <reasoning>\n",
        "        {reasoning}\n",
        "    </reasoning>\n",
        "\n",
        "    Here is the short-term memory (result of previous subtasks):\n",
        "    <memory>\n",
        "        {json.dumps(memory)}\n",
        "    </memory>\n",
        "\n",
        "    Determine the most appropriate action to take:\n",
        "        - If the task requires a calculation or verification through code, use the 'generate_code' action.\n",
        "        - If the task requires reasoning without code or calculations, use the 'reasoning' action.\n",
        "\n",
        "    Consider the overall goal and previous results when determining the action.\n",
        "\n",
        "    Return the result as a JSON object with 'action' and 'parameters' keys.  The 'parameters' key should always be a dictionary with 'prompt' as a key.\n",
        "\n",
        "    Example JSON responses:\n",
        "\n",
        "    {{\n",
        "        \"action\": \"generate_code\",\n",
        "        \"parameters\": {{\"prompt\": \"Write a function to calculate the area of a circle.\"}}\n",
        "    }}\n",
        "\n",
        "    {{\n",
        "        \"action\": \"reasoning\",\n",
        "        \"parameters\": {{\"prompt\": \"Explain how to complete the subtask.\"}}\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "    return response\n",
        "\n",
        "def evaluator(user_query: str, subtasks: List[str], current_subtask: str, action_info: Dict[str, Any], execution_result: Dict[str, Any], memory: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
        "\n",
        "    The subtasks to complete to answer the user's query are:\n",
        "    <subtasks>\n",
        "        {json.dumps(subtasks)}\n",
        "    </subtasks>\n",
        "\n",
        "    The current subtask to complete is:\n",
        "    <current_subtask>\n",
        "        {current_subtask}\n",
        "    </current_subtask>\n",
        "\n",
        "    The result of the current subtask is:\n",
        "    <result>\n",
        "        {action_info}\n",
        "    </result>\n",
        "\n",
        "    The execution result of the current subtask is:\n",
        "    <execution_result>\n",
        "        {execution_result}\n",
        "    </execution_result>\n",
        "\n",
        "    Here is the short-term memory (result of previous subtasks):\n",
        "    <memory>\n",
        "        {json.dumps(memory)}\n",
        "    </memory>\n",
        "\n",
        "    Evaluate if the result is a reasonable answer for the current subtask, and makes sense in the context of the overall query.\n",
        "\n",
        "    Return a JSON object with 'evaluation' (string) and 'retry' (boolean) keys.\n",
        "\n",
        "    Example JSON response:\n",
        "    {{\n",
        "        \"evaluation\": \"The result is a reasonable answer for the current subtask.\",\n",
        "        \"retry\": false\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "    return response\n",
        "\n",
        "def final_answer_extractor(user_query: str, subtasks: List[str], memory: List[Dict[str, Any]]) -> str:\n",
        "    prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
        "\n",
        "    The subtasks completed to answer the user's query are:\n",
        "    <subtasks>\n",
        "        {json.dumps(subtasks)}\n",
        "    </subtasks>\n",
        "\n",
        "    The memory of the thought process (short-term memory) is:\n",
        "    <memory>\n",
        "        {json.dumps(memory)}\n",
        "    </memory>\n",
        "\n",
        "    Extract the final answer that directly addresses the user's query, from the memory.\n",
        "    Provide only the essential information without unnecessary explanations.\n",
        "\n",
        "    Return a JSON object with 'finalAnswer' as a key.\n",
        "\n",
        "    Here is an example JSON response:\n",
        "    {{\n",
        "        \"finalAnswer\": \"The final answer to the user's query, addressing all aspects of the question, based on the memory provided\",\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "    return response[\"finalAnswer\"]\n",
        "\n",
        "\n",
        "def generate_and_execute_code(prompt: str, user_query: str, memory: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    code_generation_prompt = f\"\"\"\n",
        "\n",
        "    Generate Python code to implement the following task: '{prompt}'\n",
        "\n",
        "    Here is the overall goal of answering the user's query: '{user_query}'\n",
        "\n",
        "    Keep in mind the results of the previous subtasks, and use them to complete the current subtask.\n",
        "    <memory>\n",
        "        {json.dumps(memory)}\n",
        "    </memory>\n",
        "\n",
        "\n",
        "    Here are the guidelines for generating the code:\n",
        "        - Return only the Python code, without any explanations or markdown formatting.\n",
        "        - The code should always print or return a value\n",
        "        - Don't include any backticks or code blocks in your response. Do not include ```python or ``` in your response, just give me the code.\n",
        "        - Do not ever use the input() function in your code, use defined values instead.\n",
        "        - Do not ever use NLP techniques in your code, such as importing nltk, spacy, or any other NLP library.\n",
        "        - Don't ever define a function in your code, just generate the code to execute the subtask.\n",
        "        - Don't ever provide the execution result in your response, just give me the code.\n",
        "        - If your code needs to import any libraries, do it within the code itself.\n",
        "        - The code should be self-contained and ready to execute on its own.\n",
        "        - Prioritize explicit details over assumed patterns\n",
        "        - Avoid unnecessary complications in problem-solving\n",
        "    \"\"\"\n",
        "\n",
        "    generated_code = get_llm_response(\"groq\", code_generation_prompt)\n",
        "\n",
        "\n",
        "    print(f\"\\n\\nGenerated Code: start|{generated_code}|END\\n\\n\")\n",
        "\n",
        "    old_stdout = sys.stdout\n",
        "    sys.stdout = buffer = io.StringIO()\n",
        "\n",
        "    exec(generated_code)\n",
        "\n",
        "    sys.stdout = old_stdout\n",
        "    output = buffer.getvalue()\n",
        "\n",
        "    print(f\"\\n\\n***** Execution Result: |start|{output.strip()}|end| *****\\n\\n\")\n",
        "\n",
        "    return {\n",
        "        \"generated_code\": generated_code,\n",
        "        \"execution_result\": output.strip()\n",
        "    }\n",
        "\n",
        "def executor(action: str, parameters: Dict[str, Any], user_query: str, memory: List[Dict[str, Any]]) -> Any:\n",
        "    if action == \"generate_code\":\n",
        "        print(f\"Generating code for: {parameters['prompt']}\")\n",
        "        return generate_and_execute_code(parameters[\"prompt\"], user_query, memory)\n",
        "    elif action == \"reasoning\":\n",
        "        return parameters[\"prompt\"]\n",
        "    else:\n",
        "        return f\"Action '{action}' not implemented\"\n",
        "\n",
        "\n",
        "\n",
        "def autonomous_agent(user_query: str) -> List[Dict[str, Any]]:\n",
        "    memory = []\n",
        "    subtasks = planner(user_query)\n",
        "\n",
        "    print(\"User Query:\", user_query)\n",
        "    print(f\"Subtasks: {subtasks}\")\n",
        "\n",
        "    for subtask in subtasks:\n",
        "        max_retries = 1\n",
        "        for attempt in range(max_retries):\n",
        "\n",
        "            reasoning = reasoner(user_query, subtasks, subtask, memory)\n",
        "            action_info = actioner(user_query, subtasks, subtask, reasoning, memory)\n",
        "\n",
        "            print(f\"\\n\\n ****** Action Info: {action_info} ****** \\n\\n\")\n",
        "\n",
        "            execution_result = executor(action_info[\"action\"], action_info[\"parameters\"], user_query, memory)\n",
        "\n",
        "            print(f\"\\n\\n ****** Execution Result: {execution_result} ****** \\n\\n\")\n",
        "            evaluation = evaluator(user_query, subtasks, subtask, action_info, execution_result, memory)\n",
        "\n",
        "            step = {\n",
        "                \"subtask\": subtask,\n",
        "                \"reasoning\": reasoning,\n",
        "                \"action\": action_info,\n",
        "                \"evaluation\": evaluation\n",
        "            }\n",
        "            memory.append(step)\n",
        "\n",
        "            print(f\"\\n\\nSTEP: {step}\\n\\n\")\n",
        "\n",
        "            if not evaluation[\"retry\"]:\n",
        "                break\n",
        "\n",
        "            if attempt == max_retries - 1:\n",
        "                print(f\"Max retries reached for subtask: {subtask}\")\n",
        "\n",
        "    final_answer = final_answer_extractor(user_query, subtasks, memory)\n",
        "    return final_answer\n"
      ],
      "metadata": {
        "id": "BpZ1hRaiD-hS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How many r's are in strawberry?\"\n",
        "result = autonomous_agent(query)\n",
        "print(\"FINAL ANSWER: \", result)"
      ],
      "metadata": {
        "id": "MUMjfWwcK7ig"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "BWjY2S2mxh3w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "b020154f-e0ed-4336-fbc5-496dea0e672e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"There are 3 r's in 'strawberry'.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1BFyiCpRxh6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI o1-preview model getting trivial questions wrong\n",
        "\n",
        "- [Link to Reddit post](https://www.reddit.com/r/ChatGPT/comments/1ff9w7y/new_o1_still_fails_miserably_at_trivial_questions/)\n",
        "- Links to ChatGPT threads: [(1)](https://chatgpt.com/share/66f21757-db2c-8012-8b0a-11224aed0c29), [(2)](https://chatgpt.com/share/66e3c1e5-ae00-8007-8820-fee9eb61eae5)\n",
        "- [Improving reasoning in LLMs through thoughtful prompting](https://www.reddit.com/r/singularity/comments/1fdhs2m/did_i_just_fix_the_data_overfitting_problem_in/?share_id=6DsDLJUu1qEx_bsqFDC8a&utm_content=2&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1)\n"
      ],
      "metadata": {
        "id": "MrSeaoPqq1qY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"The surgeon, who is the boy's father, says, 'I can't operate on this boy, he's my son!' Who is the surgeon to the boy?\"\n",
        "result = get_llm_response(\"openai\", query)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rM4bcny1JliP",
        "outputId": "7a5eb793-e9ef-4eaf-8139-122983e04145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The surgeon is the boy's **mother**.\n",
            "\n",
            "This classic riddle challenges the assumption that a surgeon must be male. Here's the breakdown:\n",
            "\n",
            "- **The boy's father** is one person—the man who raised him.\n",
            "- When the surgeon says, \"I can't operate on this boy; he's my son,\" it implies a parental relationship.\n",
            "- Since we've already accounted for the father, the other parent is the **mother**.\n",
            "- Therefore, the surgeon is the boy's mother.\n",
            "\n",
            "The riddle highlights how stereotypes can lead us to overlook obvious answers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"The surgeon, who is the boy's father, says, 'I can't operate on this boy, he's my son!' Who is the surgeon to the boy?\"\n",
        "result = autonomous_agent(query)\n",
        "print(\"FINAL ANSWER: \", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXAPpgkpK7lG",
        "outputId": "e8b18ff3-41f3-4554-a397-c1957689ecae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'subtasks': ['reasoning: Determine the role of the surgeon in relation to the boy', 'generate_code: Identify the relation between the surgeon and the boy using family relationships, assuming the surgeon is the father and the boy is his son', 'reasoning: Based on the established relation, conclude who the surgeon is to the boy']}\n",
            "User Query: The surgeon, who is the boy's father, says, 'I can't operate on this boy, he's my son!' Who is the surgeon to the boy?\n",
            "Subtasks: ['reasoning: Determine the role of the surgeon in relation to the boy', 'generate_code: Identify the relation between the surgeon and the boy using family relationships, assuming the surgeon is the father and the boy is his son', 'reasoning: Based on the established relation, conclude who the surgeon is to the boy']\n",
            "\n",
            "\n",
            " ****** Action Info: {'action': 'reasoning', 'parameters': {'prompt': \"Based on the sentence 'The surgeon, who is the boy's father, says, 'I can't operate on this boy, he's my son!' Identify the relation between the surgeon and the boy, assuming the surgeon is the father and the boy is his son.\"}} ****** \n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ****** Execution Result: Based on the sentence 'The surgeon, who is the boy's father, says, 'I can't operate on this boy, he's my son!' Identify the relation between the surgeon and the boy, assuming the surgeon is the father and the boy is his son. ****** \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "STEP: {'subtask': 'reasoning: Determine the role of the surgeon in relation to the boy', 'reasoning': 'Analyze the given sentence to identify roles of characters. Identify the boy as the son and the surgeon as the father.', 'action': {'action': 'reasoning', 'parameters': {'prompt': \"Based on the sentence 'The surgeon, who is the boy's father, says, 'I can't operate on this boy, he's my son!' Identify the relation between the surgeon and the boy, assuming the surgeon is the father and the boy is his son.\"}}, 'evaluation': {'evaluation': 'The result correctly identifies the task to determine the relation between the surgeon and the boy based on the provided sentence and the assumptions.', 'retry': False}}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ****** Action Info: {'action': 'generate_code', 'parameters': {'prompt': 'Identify the relation between the surgeon and the boy using family relationships.'}} ****** \n",
            "\n",
            "\n",
            "Generating code for: Identify the relation between the surgeon and the boy using family relationships.\n",
            "\n",
            "\n",
            "Generated Code: start|import numpy as np\n",
            "\n",
            "surgeon_role = \"father\"\n",
            "boy_role = \"son\"\n",
            "\n",
            "relation = {\n",
            "    'father': np.array(['son']),\n",
            "    'son': np.array(['father']),\n",
            "    'mother': np.array(['son','daughter']),\n",
            "    'daughter': np.array(['father','mother'])\n",
            "}\n",
            "\n",
            "relation_dict = {\"father\": \"son\", \"son\": \"father\"}\n",
            "\n",
            "print(relation_dict[surgeon_role])|END\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "***** Execution Result: |start|son|end| *****\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ****** Execution Result: {'generated_code': 'import numpy as np\\n\\nsurgeon_role = \"father\"\\nboy_role = \"son\"\\n\\nrelation = {\\n    \\'father\\': np.array([\\'son\\']),\\n    \\'son\\': np.array([\\'father\\']),\\n    \\'mother\\': np.array([\\'son\\',\\'daughter\\']),\\n    \\'daughter\\': np.array([\\'father\\',\\'mother\\'])\\n}\\n\\nrelation_dict = {\"father\": \"son\", \"son\": \"father\"}\\n\\nprint(relation_dict[surgeon_role])', 'execution_result': 'son'} ****** \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "STEP: {'subtask': 'generate_code: Identify the relation between the surgeon and the boy using family relationships, assuming the surgeon is the father and the boy is his son', 'reasoning': \"Given the memory of the boy as the surgeon's son, we can use family relationships to establish that the surgeon is the father. We can use a simple inheritance relationship to represent this and get the result of the current subtask.\", 'action': {'action': 'generate_code', 'parameters': {'prompt': 'Identify the relation between the surgeon and the boy using family relationships.'}}, 'evaluation': {'evaluation': 'The generated code correctly determines the relation between the surgeon and the boy as father and son respectively and aligns with the overall goal.', 'retry': False}}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ****** Action Info: {'action': 'reasoning', 'parameters': {'prompt': \"Explain who the surgeon is to the boy given that the surgeon is the boy's father.\"}} ****** \n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ****** Execution Result: Explain who the surgeon is to the boy given that the surgeon is the boy's father. ****** \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "STEP: {'subtask': 'reasoning: Based on the established relation, conclude who the surgeon is to the boy', 'reasoning': \"We can conclude who the surgeon is to the boy based on the previously established relation. Given the surgeon as the boy's father, we can now state that the surgeon is the boy's parent specifically being his father.\", 'action': {'action': 'reasoning', 'parameters': {'prompt': \"Explain who the surgeon is to the boy given that the surgeon is the boy's father.\"}}, 'evaluation': {'evaluation': \"The result is correct as the surgeon is the boy's father based on the established relation of father-son, and aligns with the overall goal of determining the surgeon's relationship to the boy.\", 'retry': False}}\n",
            "\n",
            "\n",
            "FINAL ANSWER:  The surgeon is the boy's father.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The Bear Puzzle: A hunter leaves his tent. He travels 5 steps due south, 5 steps due east, and 5 steps due north. He arrives back at his tent, and sees a brown bear inside it. What color was the bear?\"\n",
        "\n",
        "result = get_llm_response(\"openai\", prompt)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjKgUDihI4lA",
        "outputId": "90b37e7d-18d8-4629-8dc0-23204e2c37ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To solve this puzzle, we need to analyze the hunter's movements and determine where on Earth this scenario could occur.\n",
            "\n",
            "**Hunter's Movements:**\n",
            "1. The hunter starts at his tent.\n",
            "2. He travels 5 steps due south.\n",
            "3. He travels 5 steps due east.\n",
            "4. He travels 5 steps due north.\n",
            "5. He arrives back at his tent.\n",
            "\n",
            "**Analysis:**\n",
            "\n",
            "- **Normal Geometry:** In regular Euclidean geometry, moving south, then east, then north of equal distances would not bring you back to your starting point unless special conditions are met.\n",
            "  \n",
            "- **Possible Locations:**\n",
            "  - **Near the North Pole:** At the North Pole, all directions point south. If you move 5 steps south from the North Pole, you are at a latitude where you can move east in a circle around the pole. Since the meridians converge at the poles, moving east at this latitude keeps you at a constant distance from the pole. Then, moving 5 steps north brings you back to the North Pole.\n",
            "\n",
            "- **Implication for the Bear:**\n",
            "  - **Polar Bears:** Polar bears are native to the Arctic region around the North Pole and are known for their white fur.\n",
            "\n",
            "**Conclusion:**\n",
            "\n",
            "Given that the only terrestrial location where the described path would return the hunter to his starting point is the North Pole—and the fact that polar bears (which are white) are found there—the color of the bear must be **white**.\n",
            "\n",
            "**Answer:** White.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The Bear Puzzle: A hunter leaves his tent. He travels 5 steps due south, 5 steps due east, and 5 steps due north. He arrives back at his tent, and sees a brown bear inside it. What color was the bear?\"\n",
        "\n",
        "result = autonomous_agent(prompt)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JNPSaGWKF5s",
        "outputId": "8e347c6c-dd25-4073-b6a9-4181b67cc824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'subtasks': [\"reasoning: Understand that the hunter's movements have no impact on the bear's initial color, since the puzzle doesn't mention the bear's movements or the color being changed by any physical means\", 'reasoning: Recognize that the puzzle is presenting an impossible scenario, where the hunter returns to an initially inaccessible location', \"reasoning: Deduce that the bear must have been placed there by an outside agent (e.g., another person) since the hunter cannot physically affect the bear's position in this way\", \"reasoning: Conclude that since there's no logical connection between the hunter's actions and the bear's initial color, the only solution is that the puzzle provided an outside piece of information that determined the bear's color\"]}\n",
            "User Query: The Bear Puzzle: A hunter leaves his tent. He travels 5 steps due south, 5 steps due east, and 5 steps due north. He arrives back at his tent, and sees a brown bear inside it. What color was the bear?\n",
            "Subtasks: [\"reasoning: Understand that the hunter's movements have no impact on the bear's initial color, since the puzzle doesn't mention the bear's movements or the color being changed by any physical means\", 'reasoning: Recognize that the puzzle is presenting an impossible scenario, where the hunter returns to an initially inaccessible location', \"reasoning: Deduce that the bear must have been placed there by an outside agent (e.g., another person) since the hunter cannot physically affect the bear's position in this way\", \"reasoning: Conclude that since there's no logical connection between the hunter's actions and the bear's initial color, the only solution is that the puzzle provided an outside piece of information that determined the bear's color\"]\n",
            "\n",
            "\n",
            " ****** Action Info: {'action': 'reasoning', 'parameters': {'prompt': \"Carefully read the puzzle statement to identify implications of the hunter's movements and the absence of bear movements or color-changing mechanisms. Explain how this lack of logical connection affects the determination of the bear's color.\"}} ****** \n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ****** Execution Result: Carefully read the puzzle statement to identify implications of the hunter's movements and the absence of bear movements or color-changing mechanisms. Explain how this lack of logical connection affects the determination of the bear's color. ****** \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "STEP: {'subtask': \"reasoning: Understand that the hunter's movements have no impact on the bear's initial color, since the puzzle doesn't mention the bear's movements or the color being changed by any physical means\", 'reasoning': \"Read the puzzle statement carefully to identify any implications of the hunter's movements and the absence of bear movements or color-changing mechanisms. Identify the lack of logical connection between the hunter's actions and the bear's color.\", 'action': {'action': 'reasoning', 'parameters': {'prompt': \"Carefully read the puzzle statement to identify implications of the hunter's movements and the absence of bear movements or color-changing mechanisms. Explain how this lack of logical connection affects the determination of the bear's color.\"}}, 'evaluation': {'evaluation': \"The result is a good start, but it's slightly too narrowly focused on the physical impact of the hunter's movements on the bear. Consider broadening the explanation to include the absence of any mechanism that would change the bear's color.\", 'retry': False}}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ****** Action Info: {'action': 'reasoning', 'parameters': {'prompt': \"Explain why the hunter's return to his inaccessible location within the given steps is impossible and how to proceed with the analysis of the puzzle.\"}} ****** \n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ****** Execution Result: Explain why the hunter's return to his inaccessible location within the given steps is impossible and how to proceed with the analysis of the puzzle. ****** \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "STEP: {'subtask': 'reasoning: Recognize that the puzzle is presenting an impossible scenario, where the hunter returns to an initially inaccessible location', 'reasoning': \"Consider analyzing the hunter's steps to understand why he returns to his initially inaccessible tent. This will help in identifying the impossible scenario presented in the puzzle.\", 'action': {'action': 'reasoning', 'parameters': {'prompt': \"Explain why the hunter's return to his inaccessible location within the given steps is impossible and how to proceed with the analysis of the puzzle.\"}}, 'evaluation': {'evaluation': 'The result correctly identifies the impossible scenario presented by the puzzle and provides a clear prompt for further analysis.', 'retry': False}}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ****** Action Info: {'action': 'reasoning', 'parameters': {'prompt': \"Explain how the presence of another agent affects the initial color of the bear, considering the puzzle's presentation of an impossible scenario where the hunter returns to his initially inaccessible tent.\"}} ****** \n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ****** Execution Result: Explain how the presence of another agent affects the initial color of the bear, considering the puzzle's presentation of an impossible scenario where the hunter returns to his initially inaccessible tent. ****** \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "STEP: {'subtask': \"reasoning: Deduce that the bear must have been placed there by an outside agent (e.g., another person) since the hunter cannot physically affect the bear's position in this way\", 'reasoning': \"Since the hunter's movements have no physical impact on the bear, and the puzzle presents an impossible scenario, consider deducing that the bear's placement is the result of external intervention. Focus on explaining how the presence of another agent affects the initial color of the bear.\", 'action': {'action': 'reasoning', 'parameters': {'prompt': \"Explain how the presence of another agent affects the initial color of the bear, considering the puzzle's presentation of an impossible scenario where the hunter returns to his initially inaccessible tent.\"}}, 'evaluation': {'evaluation': 'The result is correct and makes sense in the context of the overall query. It logically follows from the previous subtasks and sets the stage for the next step in reasoning.', 'retry': False}}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ****** Action Info: {'action': 'reasoning', 'parameters': {'prompt': \"Explain how to proceed with the current subtask, clearly stating that since the bear's placement is the result of external intervention, its color must have been specified within the puzzle itself.\"}} ****** \n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ****** Execution Result: Explain how to proceed with the current subtask, clearly stating that since the bear's placement is the result of external intervention, its color must have been specified within the puzzle itself. ****** \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "STEP: {'subtask': \"reasoning: Conclude that since there's no logical connection between the hunter's actions and the bear's initial color, the only solution is that the puzzle provided an outside piece of information that determined the bear's color\", 'reasoning': \"To complete the current subtask, clearly state that since the bear's placement is the result of external intervention, its color must have been specified within the puzzle itself. Evaluate the puzzle's statement for any explicit mentions of the bear's color, which will provide the solution to the query.\", 'action': {'action': 'reasoning', 'parameters': {'prompt': \"Explain how to proceed with the current subtask, clearly stating that since the bear's placement is the result of external intervention, its color must have been specified within the puzzle itself.\"}}, 'evaluation': {'evaluation': \"The result correctly identifies the necessary condition for determining the bear's color, which is that the puzzle itself provides the color information.\", 'retry': False}}\n",
            "\n",
            "\n",
            "Since the puzzle doesn't establish any logical connection between the hunter's actions and the bear's initial color, and the bear's presence must be attributed to external intervention, the bear's color was specified in the puzzle itself.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2ciZ_VUNNJRh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}